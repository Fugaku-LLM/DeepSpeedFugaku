# 1.3B model benchmarking results

## Overview
### Model hyperparameters
- --num-layers 24 
- --hidden-size 2048 
- --num-attention-heads 32 
### Others
- GPT-Neo 1.3B (362k steps, Sequence Length=1024, GBS=1024) 
  - 0.5 Sec/it ==> 2.09 days
  - 1 Sec/it ==> 4.18 days
  - 2 Sec/it ==> 8.36 days
  - 3 Sec/it ==> 12.54 days
  - 10 Sec/it ==> 41.8 days

### Notations
- MBS = micro batch size
- GBS = global batch size
- Sec/it = seconds per iteration 
- Est. Aggr. PetaFLOPs = TFLOPs * Nodes / 1024

## Experiments

### Sequence Length=1024 w\o activation checkpointing, PyTorch 1.10
| Nodes | Size | DP | TP | PP | MBS |  GBS | Mem  | Sec/it | TFLOPs |Est. Aggr. PetaFLOPs| Notes |
| ----: | ---: | -: | -: | -: | --: |  --: | ---: | -----: | -----: | ---: | ----: |
|   4 | 1.3B |1 |  4 |  1 |   1 | 1024 |18870.0 MiB | 2533.0 |  0.86   |  - | 02-15|
|   64 | 1.3B |16 |  4 |  1 |   1 | 1024 | 18502.5 MiB | 162.5 |  0.84  |  - | 02-15|
|   256 | 1.3B |64 |  4 |  1 |   1 | 1024 | - MiB | 20033.7 MiB |   14.5  |  0.59 | 02-15|
|   1024 | 1.3B | 256 |  4 |  1 |   1 | 1024 | - MiB | - |  -   |  - | 02-15|
|   2048 | 1.3B | 512 |  4 |  1 |   1 | 1024 | - MiB | - |  -   |  - | 02-15|
|   4096 | 1.3B | 1024 |  4 |  1 |   1 | 1024 | - MiB | - |  -   |  - | 02-15|
|   4096 (2.2Hz) | 1.3B | 1024 |  4 |  1 |   1 | 1024 | - MiB | - |  -   |  - | -|
|   8192 | 1.3B | 1024 |  4 |  1 |   1 | 1024 | - MiB | - |  -   |  - | -|

### Sequence Length=1024 \w activation checkpointing, PyTorch 1.7

| Nodes | Size | DP | TP | PP | MBS |  GBS | Mem  | Sec/it | TFLOPs |Est. Aggr. PetaFLOPs| Notes |
| ----: | ---: | -: | -: | -: | --: |  --: | ---: | -----: | -----: | ---: | ----: |
|   1 | 1.3B |1 |  1 |  1 |   1 | 1024 |28406.7 MiB | 11727.9 |  0.99 | 0.001 | 02-14 |
|   1 (2.2Hz) | 1.3B | 1|  1 |  1 |   1 | 1024 |28356.1 MiB | 10532.4 |  1.10 |  0.001 | 02-15 |
|   1  | 1.3B |1 |  1 |  1 |   2 | 1024 |- MiB | - |  1.18^ |  - | 02-15 |
|   4  | 1.3B |1 |  4 |  1 |   1 | 1024 |- MiB | - |  0.69 |  - | 02-15|
|   4  | 1.3B |1 |  4 |  1 |   2 | 1024 |10480.7 MiB | 3231.4 |  0.90 |  0.003 | 02-15 |
|   4  | 1.3B |1 |  4 |  1 |   2 | 1024 |10480.7 MiB | 3231.4 |  0.90 |  0.003 | 02-15 |
|   64 | 1.3B |64 |  1 |  1 |   1 | 1024 |28828.5 MiB^ | 210.0^ |  0.86^ |  0.05^  | 02-15|

### Sequence Length=1024 w/o activation checkpointing, PyTorch 1.10
| Nodes | Size | DP | TP | PP | MBS |  GBS | Mem  | Sec/it | TFLOPs |Est. Aggr. PetaFLOPs| Notes |
| ----: | ---: | -: | -: | -: | --: |  --: | ---: | -----: | -----: | ---: | ----: |
|   4 | 1.3B |1 |  4 |  1 |   1 | 1024 | - MiB | 2857.8 |  0.76 |  - | 02-15|
|   4 (2.2Hz, LD_PRELOAD=libtcmalloc.so, "retention_state=0")| 1.3B |1 |  4 |  1 |   1 | 1024 | - MiB | - |  - |  - | 02-15|



### Sequence Length=2048 

## Comments
- This is blocking due to simulatenous use of TP and DP hangs.
