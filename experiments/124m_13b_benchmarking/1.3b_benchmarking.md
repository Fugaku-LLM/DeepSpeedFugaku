# 1.3B model benchmarking results

## Overview
### Model hyperparameters
- --num-layers 24 
- --hidden-size 2048 
- --num-attention-heads 32 

### Notations
- MBS = micro batch size
- GBS = global batch size
- Sec/it = seconds per iteration 
- Est. Aggr. PetaFLOPs = TFLOPs * Nodes / 1024

## Experiments

### Sequence Length=1024 \w activation checkpointing

| Nodes | Size | DP | TP | PP | MBS |  GBS | Mem  | Sec/it | TFLOPs |Est. Aggr. PetaFLOPs| Notes |
| ----: | ---: | -: | -: | -: | --: |  --: | ---: | -----: | -----: | ---: | ----: |
|   1 | 1.3B |1 |  1 |  1 |   1 | 1024 |28406.7 MiB | 11727.9 |  0.99 | 0.001 | 02-14 |
|   1 (2.2Hz) | 1.3B | 1|  1 |  1 |   1 | 1024 |28356.1 MiB | 10532.4 |  1.10 |  0.001 | 02-15 |
|   1  | 1.3B |1 |  1 |  1 |   2 | 1024 |- MiB | - |  1.18^ |  - | 02-15 |
|   2  | 1.3B |1 |  2 |  1 |   2 | 1024 |16671.8 MiB |4858.2 |  1.20 |  0.002 | 02-15 |
|   4  | 1.3B |1 |  4 |  1 |   2 | 1024 |10480.7 MiB | 3231.4 |  0.90 |  0.003 | 02-15 |
|   64 | 1.3B |64 |  1 |  1 |   1 | 1024 |28828.5 MiB^ | 210.0^ |  0.86^ |  0.05^  | 02-15|
|  128 | 1.3B |64 |  1 |  2 |   1 | 1024 |- MiB | - |  - |  - | -|
|  128 | 1.3B |64 |  1 |  2 |   2 | 1024 |- MiB | - |  - |  - | -|
|  128 | 1.3B |64 |  1 |  2 |   4 | 1024 |- MiB | - |  - |  - | -|
|  128 | 1.3B |64 |  2 |  1 |   1 | 1024 |- MiB | - |  - |  - | -|
|  512 | 1.3B | 256 |  2 |  1 |   1 | 1024 |- MiB | - |  - |  - | -|
| 1024 | 1.3B | 512 |  2 |  1 |   1 | 1024 |- MiB | - |  - |  - | -|
| 2048 | 1.3B |1024 |  2 |  1 |   1 | 1024 |- MiB | - |  - |  - | -|
| 4096 | 1.3B |2048 |  2 |  1 |   1 | 1024 |- MiB | - |  - |  - | -|
| 8192 | 1.3B |2048 |  4 |  1 |   1 | 1024 |- MiB | - |  - |  - | -|

### Sequence Length=1024 w/o activation checkpointing
| Nodes | Size | DP | TP | PP | MBS |  GBS | Mem  | Sec/it | TFLOPs |Est. Aggr. PetaFLOPs| Notes |
| ----: | ---: | -: | -: | -: | --: |  --: | ---: | -----: | -----: | ---: | ----: |
|   4 | 1.3B |1 |  4 |  1 |   1 | 1024 | - MiB | - |  - |  - | -|

### Sequence Length=2048 

## Comments
- This is blocking due to simulatenous use of TP and DP hangs.
