# Tokenizer ver.2
## 種類

| モデル名 | 語彙の規模 (コード) | 語彙の規模 (英語) | 語彙の規模 (日本語) | 語彙の規模 (マージ後) |
| --- | --- | --- | --- | --- |
| code20k_en40k_ja80k.ver2.model | 20,000 | 40,000 | 80,000 | 117,813 |
| code20k_en60k_ja100k.ver2.model | 20,000 | 60,000 | 100,000 | 152,401 |
| code20k_en80k_ja120k.ver2.model | 30,000 | 80,000 | 120,000 | 194,571 |

（上記モデルと同じ語彙の規模のvanilla SentencePieceも追加予定）

## 学習元ファイル
- code: Stackから1GBを乱択
- en: WikipediaとPileから1GBずつを乱択
- ja: WikipediaとmC4から1GBずつを乱択

## 改行の取り扱い
`\n`が語彙に入っているため，そのまま扱えます．

```
>>> import sentencepiece as spm
>>> sp = spm.SentencePieceProcessor()
>>> sp.load('code20k_en40k_ja80k.ver2.model')
>>> sp.encode_as_pieces('Hello World!\nhow are you?')
['▁', 'hello', '▁world', '!', '\n', 'how', '▁are', '▁you', '?']

>>> sp.encode_as_pieces('こんにちは世界！\n元気ですか？')
['▁', 'こんにちは', '世界', '！', '\n', '元気', 'で', 'すか', '？']
```

## スペース記号の取り扱い
24個までのスペース連続はスコア`0.0`で語彙に入っています．

```
>>> sp.encode_as_pieces(' ' * 4 + 'def init():')
['▁▁▁▁▁', 'def', '▁', 'init', '(', ')', ':']
>>> sp.encode_as_pieces(' ' * 10 + 'def init():')
['▁▁▁▁▁▁▁▁▁▁▁', 'def', '▁', 'init', '(', ')', ':']
>>> sp.encode_as_pieces(' ' * 20 +'def init():')
['▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁', 'def', '▁', 'init', '(', ')', ':']
>>> sp.encode_as_pieces(' ' * 30 +'def init():')
['▁▁▁▁▁▁▁', '▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁', 'def', '▁', 'init', '(', ')', ':']
```

## Special Tokenの取り扱い
以下の290個のトークンはスコア `0.0` で語彙に入っています．

```
<unk>   0.0
<s> 0.0
</s>    0.0
<mask>  0.0
<pad>   0.0
<CLS>   0.0
<SEP>   0.0
<EOD>   0.0
▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁	0.0
▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁	0.0
▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁	0.0
▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁	0.0
▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁	0.0
▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁	0.0
▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁	0.0
▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁	0.0
▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁	0.0
▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁	0.0
▁▁▁▁▁▁▁▁▁▁▁▁▁▁	0.0
▁▁▁▁▁▁▁▁▁▁▁▁▁	0.0
▁▁▁▁▁▁▁▁▁▁▁▁	0.0
▁▁▁▁▁▁▁▁▁▁▁	0.0
▁▁▁▁▁▁▁▁▁▁	0.0
▁▁▁▁▁▁▁▁▁	0.0
▁▁▁▁▁▁▁▁	0.0
▁▁▁▁▁▁▁	0.0
▁▁▁▁▁▁	0.0
▁▁▁▁▁	0.0
▁▁▁▁	0.0
▁▁▁	0.0
▁▁	0.0
▁	0.0
\n    0.0
<MASK>  0.0
<0x00>  0.0
<0x01>  0.0
<0x02>  0.0
...
<0xFD>  0.0
<0xFE>  0.0
<0xFF>  0.0
```

## 分割結果比較
（追記予定）
